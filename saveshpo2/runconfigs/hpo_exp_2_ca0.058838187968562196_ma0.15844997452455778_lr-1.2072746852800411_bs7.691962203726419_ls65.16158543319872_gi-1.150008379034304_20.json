{"name": "hpo_exp_2_ca0.058838187968562196_ma0.15844997452455778_lr-1.2072746852800411_bs7.691962203726419_ls65.16158543319872_gi-1.150008379034304_20", "VISIBLE_POSITIONS": "<function visible_positions_13 at 0x7f7d6b2d75b0>", "EPOCHS": "2000", "SEED": "20", "PLOTTING": "False", "PREDICT_GOAL_ONLY_AT_END": "True", "COM_ALPHA": "0.058838187968562196", "WORLD_GENERATOR": "choice", "OBJECT_COLOR_RANGE": "(None, 8)", "NUMBER_OF_AGENTS": "2", "NUMBER_COMMUNICATION_CHANNELS": "2", "LEARNING_RATE": "0.06204764672742884", "BATCH_SIZE": "206", "MOV_ALPHA": "0.15844997452455778", "LAYER_SIZE": "65.16158543319872", "GAMMA": "0.9292067874193249", "compute_reward": "<environment.reward.RaceReward object at 0x7f7cd87b1a80>", "env": "<CoopGridWorld instance>", "trainer": "<training.PPO.PPOTrainer.PPOTrainer object at 0x7f7c7c501180>", "PPO_EPSILON": "0.2", "GAE_LAMBDA": "0.95", "KLD_THRESHHOLD": "0.05", "STEPS_PER_TRAJECTORIE": "1000", "ACTOR_OUTPUT_ACTIVATION": "log_softmax", "save": "<bound method PPOConfig.save of <runconfig.PPOConfig object at 0x7f7c7c1dbd60>>", "get_trainer": "<bound method PPOConfig.get_trainer of <runconfig.PPOConfig object at 0x7f7c7c1dbd60>>", "actor_output_activation": "log_softmax", "critic_output_dim": "<bound method PPOConfig.critic_output_dim of <runconfig.PPOConfig object at 0x7f7c7c1dbd60>>", "ENV_PARALLEL": "32", "FROM_SAVE": "False", "TAU": "0.005", "RECURRENT": "True", "EPSILON": "None", "SOCIAL_INFLUENCE_SAMPLE_SIZE": "30", "SOCIAL_REWARD_WEIGHT": "0", "LSTM_TIME_STEPS": "10", "MAX_REPLAY_BUFFER_SIZE": "10000", "LSTM_SIZE": "32", "NUMBER_OF_BIG_LAYERS": "2", "GRID_SIZE_RANGE": "(12, 16)", "MAX_TIME_STEP": "30", "COMMUNISM": "False", "AGENT_DROPOUT_PROBS": "0", "NUMBER_OF_OBJECTS_TO_PLACE_RANGE": "(0.2, 0.6)", "POS_REWARD": "1", "NEG_REWARD": "-0.05", "CHOICE_NEG_REWARD": "0.0", "REWARD_TYPE": "race", "XENIA_LOCK": "True", "XENIA_PERMANENCE": "True", "SIZE_VOCABULARY": "4"}