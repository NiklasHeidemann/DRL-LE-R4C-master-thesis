{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OoOEob3gB00H",
        "outputId": "84488333-92f5-4018-c88b-284b3c5d7ab8"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found existing installation: gym 0.25.2\n",
            "Uninstalling gym-0.25.2:\n",
            "  Would remove:\n",
            "    /usr/local/lib/python3.8/dist-packages/gym-0.25.2.dist-info/*\n",
            "    /usr/local/lib/python3.8/dist-packages/gym/*\n",
            "Proceed (Y/n)? y\n",
            "  Successfully uninstalled gym-0.25.2\n"
          ]
        }
      ],
      "source": [
        "!pip3 uninstall gym"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting gym[mujoco]\n",
            "  Downloading gym-0.26.2.tar.gz (721 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m721.7/721.7 KB\u001b[0m \u001b[31m14.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: numpy>=1.18.0 in /usr/local/lib/python3.8/dist-packages (from gym[mujoco]) (1.21.6)\n",
            "Requirement already satisfied: cloudpickle>=1.2.0 in /usr/local/lib/python3.8/dist-packages (from gym[mujoco]) (2.2.0)\n",
            "Requirement already satisfied: gym-notices>=0.0.4 in /usr/local/lib/python3.8/dist-packages (from gym[mujoco]) (0.0.8)\n",
            "Requirement already satisfied: importlib-metadata>=4.8.0 in /usr/local/lib/python3.8/dist-packages (from gym[mujoco]) (6.0.0)\n",
            "Collecting mujoco==2.2\n",
            "  Downloading mujoco-2.2.0-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.6 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.6/3.6 MB\u001b[0m \u001b[31m55.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting imageio>=2.14.1\n",
            "  Downloading imageio-2.25.0-py3-none-any.whl (3.4 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.4/3.4 MB\u001b[0m \u001b[31m53.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: absl-py in /usr/local/lib/python3.8/dist-packages (from mujoco==2.2->gym[mujoco]) (1.3.0)\n",
            "Requirement already satisfied: pyopengl in /usr/local/lib/python3.8/dist-packages (from mujoco==2.2->gym[mujoco]) (3.1.6)\n",
            "Collecting glfw\n",
            "  Downloading glfw-2.5.5-py2.py27.py3.py30.py31.py32.py33.py34.py35.py36.py37.py38-none-manylinux2014_x86_64.whl (207 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.5/207.5 KB\u001b[0m \u001b[31m2.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting pillow>=8.3.2\n",
            "  Downloading Pillow-9.4.0-cp38-cp38-manylinux_2_28_x86_64.whl (3.4 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.4/3.4 MB\u001b[0m \u001b[31m82.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.8/dist-packages (from importlib-metadata>=4.8.0->gym[mujoco]) (3.11.0)\n",
            "Building wheels for collected packages: gym\n",
            "  Building wheel for gym (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for gym: filename=gym-0.26.2-py3-none-any.whl size=827649 sha256=d472e54c00c541da4a17f85323f4d16f69b87aaa71bb1b194e08b4c1111f044c\n",
            "  Stored in directory: /root/.cache/pip/wheels/17/79/65/7afedc162d858b02708a3b8f7a6dd5b1000dcd5b0f894f7cc1\n",
            "Successfully built gym\n",
            "Installing collected packages: glfw, pillow, mujoco, imageio, gym\n",
            "  Attempting uninstall: pillow\n",
            "    Found existing installation: Pillow 7.1.2\n",
            "    Uninstalling Pillow-7.1.2:\n",
            "      Successfully uninstalled Pillow-7.1.2\n",
            "  Attempting uninstall: imageio\n",
            "    Found existing installation: imageio 2.9.0\n",
            "    Uninstalling imageio-2.9.0:\n",
            "      Successfully uninstalled imageio-2.9.0\n",
            "Successfully installed glfw-2.5.5 gym-0.26.2 imageio-2.25.0 mujoco-2.2.0 pillow-9.4.0\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "PIL"
                ]
              }
            }
          },
          "metadata": {}
        }
      ],
      "source": [
        "!pip3 install gym[mujoco]"
      ],
      "metadata": {
        "pycharm": {
          "name": "#%%\n"
        },
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 815
        },
        "id": "b9hF7N706aXK",
        "outputId": "ab251664-b787-4f01-d5da-cee083ac466a"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "\n",
        "\n",
        "class ExperienceReplayBuffer:\n",
        "    \"\"\"\n",
        "    Experience Replay Buffer\n",
        "    state_dims, action_dims, max_size=1000000, batch_size=256\n",
        "    :param state_dims: Dimensions of the state space\n",
        "    :param action_dims: Dimensions of the action space\n",
        "    :param max_size=1000000: Size of the replay buffer\n",
        "    :param batch_size=256: Minibatch size for each gradient update\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, state_dims, action_dims, max_size=1000000, batch_size=256):\n",
        "        self._max_size = max_size\n",
        "        self._batch_size = batch_size\n",
        "        self._size = 0\n",
        "        self._current_position = 0\n",
        "        self._state_memory = np.zeros((self._max_size, *state_dims))\n",
        "        self._state_prime_memory = np.zeros((self._max_size, *state_dims))\n",
        "        self._action_memory = np.zeros((self._max_size, action_dims))\n",
        "        self._reward_memory = np.zeros((self._max_size, 1))\n",
        "        self._done_memory = np.zeros((self._max_size, 1), dtype=bool)\n",
        "\n",
        "    def size(self):\n",
        "        return self._size\n",
        "\n",
        "    def ready(self):\n",
        "        return self._size >= self._batch_size\n",
        "\n",
        "    def add_transition(self, state, action, reward, state_, done):\n",
        "        self._state_memory[self._current_position] = state\n",
        "        self._state_prime_memory[self._current_position] = state_\n",
        "        self._action_memory[self._current_position] = action\n",
        "        self._reward_memory[self._current_position] = reward\n",
        "        self._done_memory[self._current_position] = done\n",
        "        # self.un_norm_r[self.current_position] = r\n",
        "        # self.r = (self.un_norm_r - np.mean(self.un_norm_r)) / (np.std(self.un_norm_r) + 1e-10)\n",
        "        if self._size < self._max_size:\n",
        "            self._size += 1\n",
        "        self._current_position = (self._current_position + 1) % self._max_size\n",
        "\n",
        "    def sample_batch(self):\n",
        "        batch_indices = np.random.choice(self._size, self._batch_size, replace=False)\n",
        "        states = tf.convert_to_tensor(self._state_memory[batch_indices], dtype=tf.float32)\n",
        "        states_prime = tf.convert_to_tensor(self._state_prime_memory[batch_indices], dtype=tf.float32)\n",
        "        actions = tf.convert_to_tensor(self._action_memory[batch_indices], dtype=tf.float32)\n",
        "        rewards = tf.convert_to_tensor(self._reward_memory[batch_indices], dtype=tf.float32)\n",
        "        dones = tf.convert_to_tensor(self._done_memory[batch_indices], dtype=tf.float32)\n",
        "        return states, actions, rewards, states_prime, dones\n",
        "\n",
        "\n",
        "import tensorflow as tf\n",
        "import tensorflow.keras as keras\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from tensorflow.keras.layers import Dense, Concatenate\n",
        "\n",
        "\n",
        "def create_policy_network(learning_rate, state_dim, action_dim):\n",
        "    inputs = keras.Input(shape=state_dim)\n",
        "    x = Dense(256, activation=tf.nn.relu)(inputs)\n",
        "    x = Dense(256, activation=tf.nn.relu)(x)\n",
        "    x = Dense(256, activation=tf.nn.relu)(x)\n",
        "    mu = Dense(action_dim, activation=None)(x)\n",
        "    sigma = Dense(action_dim, activation=tf.nn.softplus)(x)\n",
        "    model = keras.Model(inputs=inputs, outputs=(mu, sigma))\n",
        "    model.compile(optimizer=Adam(learning_rate=learning_rate))\n",
        "    return model\n",
        "\n",
        "\n",
        "def create_q_network(learning_rate, state_dim, action_dim):\n",
        "    inputs_s = keras.Input(shape=state_dim)\n",
        "    inputs_a = keras.Input(shape=action_dim)\n",
        "    x = Concatenate()([inputs_s, inputs_a])\n",
        "    x = Dense(256, activation=tf.nn.relu)(x)\n",
        "    x = Dense(256, activation=tf.nn.relu)(x)\n",
        "    x = Dense(256, activation=tf.nn.relu)(x)\n",
        "    out = Dense(1, activation=None)(x)\n",
        "    model = keras.Model(inputs=(inputs_s, inputs_a), outputs=out)\n",
        "    model.compile(optimizer=Adam(learning_rate=learning_rate))\n",
        "    return model\n",
        "\n",
        "\n",
        "# from ExperienceReplayBuffer import ExperienceReplayBuffer\n",
        "import tensorflow as tf\n",
        "from tensorflow import math as tfm\n",
        "from tensorflow_probability import distributions as tfd\n",
        "import numpy as np\n",
        "import datetime\n",
        "\n",
        "\n",
        "# input actions are always between (−1, 1)\n",
        "def default_scaling(actions):\n",
        "    return actions\n",
        "\n",
        "\n",
        "# input actions are always between (−1, 1)\n",
        "def multiplicative_scaling(actions, factors):\n",
        "    return actions * factors\n",
        "\n",
        "\n",
        "class Agent:\n",
        "    \"\"\"\n",
        "    Soft Actor-Critic (SAC) Agent\n",
        "    Based on pseudocode of OpenAI Spinning Up (2022) (https://spinningup.openai.com/en/latest/algorithms/sac.html)\n",
        "    and the Paper of Haarnoja et al. (2018) (https://arxiv.org/abs/1801.01290)\n",
        "\n",
        "    :param environment: The environment to learn from (conforming to the gymnasium specifics https://gymnasium.farama.org/)\n",
        "    :param state_dim: Dimensions of the state space\n",
        "    :param action_dim: Dimensions of the action space\n",
        "    :param actor_network_generator: a generator function for the actor network\n",
        "        signature: learning_rate, state_dim, action_dim -> tensorflow Model\n",
        "    :param critic_network_generator: a generator function for the critic networks (Q-networks)\n",
        "        signature: learning_rate, state_dim, action_dim -> tensorflow Model\n",
        "    :param action_scaling=default_scaling: function to scale the actions form (-1, 1)\n",
        "        to the range the environment requires\n",
        "        signature:  (action_tensor -> scaled_action_tensor)\n",
        "    :param learning_rate=0.0003: Learning rate for adam optimizer.\n",
        "        The same learning rate will be used for all networks (Q-Values, Actor)\n",
        "    :param gamma=0.99: discount factor\n",
        "    :param tau=0.005:  Polyak averaging coefficient (between 0 and 1)\n",
        "    :param alpha=0.2: Entropy regularization coefficient (between 0 and 1).\n",
        "        Controlling the exploration/exploitation trade off.\n",
        "        (inverse of reward scale in the original SAC paper)\n",
        "    :param batch_size=256: Minibatch size for each gradient update\n",
        "    :param max_replay_buffer_size=1000000: Size of the replay buffer\n",
        "    :param model_path: path to the location the model is saved and loaded from\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, environment, state_dim, action_dim,\n",
        "                 actor_network_generator, critic_network_generator, action_scaling=default_scaling,\n",
        "                 learning_rate=0.0003, gamma=0.99, tau=0.005, reward_scale=1, alpha=0.2,\n",
        "                 batch_size=256, max_replay_buffer_size=1000000, model_path=\"\"):\n",
        "        self._environment = environment\n",
        "        self._action_dim = action_dim\n",
        "        self._action_scaling = action_scaling\n",
        "        self._gamma = gamma\n",
        "        self._tau = tau\n",
        "        self._reward_scale = reward_scale\n",
        "        self._alpha = alpha\n",
        "        self._batch_size = batch_size\n",
        "        self._mse = tf.keras.losses.MeanSquaredError()\n",
        "        self._model_path = model_path\n",
        "        self._reply_buffer = ExperienceReplayBuffer(state_dim, action_dim, max_replay_buffer_size, batch_size)\n",
        "        self._actor = actor_network_generator(learning_rate)\n",
        "        self._critic_1 = critic_network_generator(learning_rate)\n",
        "        self._critic_2 = critic_network_generator(learning_rate)\n",
        "        self._critic_1_t = critic_network_generator(learning_rate)\n",
        "        self._critic_2_t = critic_network_generator(learning_rate)\n",
        "        self._wight_init()\n",
        "\n",
        "    def reply_buffer(self):\n",
        "        return self._reply_buffer\n",
        "\n",
        "    def environment(self):\n",
        "        return self._environment\n",
        "\n",
        "    def save_models(self, name):\n",
        "        self._actor.save_weights(f\"{self._model_path}actor{name}\")\n",
        "        self._critic_1.save_weights(f\"{self._model_path}critic_1{name}\")\n",
        "        self._critic_2.save_weights(f\"{self._model_path}critic_2{name}\")\n",
        "        self._critic_1_t.save_weights(f\"{self._model_path}critic_1_t{name}\")\n",
        "        self._critic_2_t.save_weights(f\"{self._model_path}critic_2_t{name}\")\n",
        "\n",
        "    def load_models(self, name):\n",
        "        self._actor.load_weights(f\"{self._model_path}actor{name}\")\n",
        "        self._critic_1.load_weights(f\"{self._model_path}critic_1{name}\")\n",
        "        self._critic_2.load_weights(f\"{self._model_path}critic_2{name}\")\n",
        "        self._critic_1_t.load_weights(f\"{self._model_path}critic_1{name}\")\n",
        "        self._critic_2_t.load_weights(f\"{self._model_path}critic_2{name}\")\n",
        "\n",
        "    def _wight_init(self):\n",
        "        self._critic_1.set_weights(self._critic_1_t.weights)\n",
        "        self._critic_2.set_weights(self._critic_2_t.weights)\n",
        "\n",
        "    def update_target_weights(self):\n",
        "        self._weight_update(self._critic_1_t, self._critic_1)\n",
        "        self._weight_update(self._critic_2_t, self._critic_2)\n",
        "\n",
        "    def _weight_update(self, target_network, network):\n",
        "        new_wights = []\n",
        "        for w_t, w in zip(target_network.weights, network.weights):\n",
        "            new_wights.append((1 - self._tau) * w_t + self._tau * w)\n",
        "        target_network.set_weights(new_wights)\n",
        "\n",
        "    def learn(self):\n",
        "        states, actions, rewards, states_prime, dones = self._reply_buffer.sample_batch()\n",
        "        self.train_step_critic(states, actions, rewards, states_prime, dones)\n",
        "        self.train_step_actor(states)\n",
        "        self.update_target_weights()\n",
        "\n",
        "    @tf.function\n",
        "    def train_step_critic(self, states, actions, rewards, states_prime, dones):\n",
        "        actions_prime, log_probs = self.sample_actions_form_policy(states_prime)\n",
        "        q1 = self._critic_1_t((states_prime, actions_prime))\n",
        "        q2 = self._critic_2_t((states_prime, actions_prime))\n",
        "        q_r = tfm.minimum(q1, q2) - self._alpha * log_probs\n",
        "        targets = self._reward_scale * rewards + self._gamma * (1 - dones) * q_r\n",
        "        self._critic_update(self._critic_1, states, actions, targets)\n",
        "        self._critic_update(self._critic_2, states, actions, targets)\n",
        "\n",
        "    def _critic_update(self, critic, states, actions, targets):\n",
        "        with tf.GradientTape() as tape:\n",
        "            q = critic((states, actions))\n",
        "            loss = 0.5 * self._mse(targets, q)\n",
        "        gradients = tape.gradient(loss, critic.trainable_variables)\n",
        "        critic.optimizer.apply_gradients(zip(gradients, critic.trainable_variables))\n",
        "\n",
        "    @tf.function\n",
        "    def train_step_actor(self, states):\n",
        "        with tf.GradientTape() as tape:\n",
        "            actions_new, log_probs = self.sample_actions_form_policy(states)\n",
        "            q1 = self._critic_1((states, actions_new))\n",
        "            q2 = self._critic_2((states, actions_new))\n",
        "            loss = tfm.reduce_mean(self._alpha * log_probs - tfm.minimum(q1, q2))\n",
        "            # equal to loss = -tfm.reduce_mean(tfm.minimum(q1, q2) - self._alpha * log_probs)\n",
        "        gradients = tape.gradient(loss, self._actor.trainable_variables)\n",
        "        self._actor.optimizer.apply_gradients(zip(gradients, self._actor.trainable_variables))\n",
        "\n",
        "    @tf.function\n",
        "    def sample_actions_form_policy(self, state):\n",
        "        mu, sigma = self._actor(state)\n",
        "        # MultivariateNormalDiag(loc=mus, scale_diag=sigmas) other option\n",
        "        distribution = tfd.Normal(mu, sigma)\n",
        "        actions = distribution.sample()\n",
        "        log_probs = distribution.log_prob(actions)\n",
        "        actions = tfm.tanh(actions)\n",
        "        log_probs -= tfm.log(1 - tfm.pow(actions, 2) + 1e-6)  # + 1e-6 because log undefined for 0\n",
        "        log_probs = tfm.reduce_sum(log_probs, axis=-1, keepdims=True)\n",
        "        return actions, log_probs\n",
        "\n",
        "    def act_deterministic(self, state):\n",
        "        actions_prime, _ = self._actor(tf.convert_to_tensor([state], dtype=tf.float32))\n",
        "        return self._act(actions_prime)\n",
        "\n",
        "    def act_stochastic(self, state):\n",
        "        actions_prime, _ = self.sample_actions_form_policy(tf.convert_to_tensor([state], dtype=tf.float32))\n",
        "        return self._act(actions_prime)\n",
        "\n",
        "    def _act(self, actions):\n",
        "        scaled_actions = self._action_scaling(actions)  # scaled actions from (-1, 1) according (to environment)\n",
        "        observation_prime, reward, done, truncated, _ = self._environment.step(scaled_actions[0])\n",
        "        return actions, observation_prime, reward, done or truncated\n",
        "\n",
        "    def train(self, epochs, environment_steps_before_training=1, training_steps_per_update=1,\n",
        "              max_environment_steps_per_epoch=None, pre_sampling_steps=1024, save_models=False):\n",
        "        \"\"\"\n",
        "        trains the SAC Agent\n",
        "        :param epochs: Number of epochs to train.\n",
        "            One epoch is finished if the agents are done ore the maximum steps are reached\n",
        "        :param environment_steps_before_training=1: Number of steps the agent takes in the environment\n",
        "            before the training cycle starts (the networks are updated after each step in the environment)\n",
        "        :param training_steps_per_update=1: Number of times the networks are update per update cykle\n",
        "        :param max_environment_steps_per_epoch=None: Maximal number of staps taken in the environment in an epoch\n",
        "        :param pre_sampling_steps=1024: Number of exploration steps sampled to the replay buffer before training starts\n",
        "        :param save_models=False: Determines if the models are saved per epoch\n",
        "        \"\"\"\n",
        "        print(f\"Random exploration for {pre_sampling_steps} steps!\")\n",
        "        observation, _ = self._environment.reset()\n",
        "        ret = 0\n",
        "        for _ in range(pre_sampling_steps):\n",
        "            actions, observation_prime, reward, done = self.act_stochastic(observation)\n",
        "            ret += reward\n",
        "            self._reply_buffer.add_transition(observation, actions, reward, observation_prime, done)\n",
        "            if done:\n",
        "                ret = 0\n",
        "                observation, _ = self._environment.reset()\n",
        "            else:\n",
        "                observation = observation_prime\n",
        "        print(\"start training!\")\n",
        "        returns = []\n",
        "        observation, _ = self._environment.reset()\n",
        "        done = 0\n",
        "        ret = 0\n",
        "        epoch = 0\n",
        "        steps = 0\n",
        "        j = 0\n",
        "        while True:\n",
        "            i = 0\n",
        "            while i < environment_steps_before_training or self._reply_buffer.size() < self._batch_size:\n",
        "                if done or (max_environment_steps_per_epoch is not None and j >= max_environment_steps_per_epoch):\n",
        "                    observation, _ = self._environment.reset()\n",
        "                    returns.append(ret)\n",
        "                    print(\"epoch:\", epoch, \"steps:\", steps, \"return:\", ret, \"avg return:\", np.average(returns[-4:]))\n",
        "                    if save_models:\n",
        "                        self.save_models(f\"SAC_{epoch}_{steps}_{ret}_{datetime.datetime.now()}\")\n",
        "                    ret = 0\n",
        "                    epoch += 1\n",
        "                    if epoch >= epochs:\n",
        "                        print(\"training finished!\")\n",
        "                        return\n",
        "                actions, observation_prime, reward, done = self.act_stochastic(observation)\n",
        "                self._reply_buffer.add_transition(observation, actions, reward, observation_prime, done)\n",
        "                observation = observation_prime\n",
        "                steps += 1\n",
        "                ret += reward\n",
        "                i += 1\n",
        "                j += 1\n",
        "            for _ in range(training_steps_per_update):\n",
        "                self.learn()\n",
        "\n",
        "\n",
        "from functools import partial\n",
        "import gym\n",
        "import tensorflow as tf\n",
        "\n",
        "# from GenericMLPs1D import create_policy_network, create_q_network\n",
        "# from SoftActorCriticAgent import Agent, multiplicative_scaling\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    tf.keras.backend.clear_session()\n",
        "    env = gym.make('InvertedPendulum-v4')\n",
        "    print(\"state_dim=\", env.observation_space.shape, \"action_dim=\", env.action_space.shape[0], \"action_scaling:\",\n",
        "          env.action_space.high)\n",
        "\n",
        "    agent = Agent(environment=env, state_dim=env.observation_space.shape, action_dim=env.action_space.shape[0],\n",
        "                  action_scaling=partial(multiplicative_scaling, factors=env.action_space.high),\n",
        "                  actor_network_generator=partial(create_policy_network, state_dim=env.observation_space.shape[0],\n",
        "                                                  action_dim=env.action_space.shape[0]),\n",
        "                  critic_network_generator=partial(create_q_network, state_dim=env.observation_space.shape[0],\n",
        "                                                   action_dim=env.action_space.shape[0]))\n",
        "    agent.train(200)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "u3kl_SYnB-E_",
        "outputId": "decbab86-5d51-4dcd-e8c2-3294926c0ab0"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "state_dim= (4,) action_dim= 1 action_scaling: [3.]\n",
            "Random exploration for 1024 steps!\n",
            "start training!\n",
            "epoch: 0 steps: 13 return: 13.0 avg return: 13.0\n",
            "epoch: 1 steps: 16 return: 3.0 avg return: 8.0\n",
            "epoch: 2 steps: 19 return: 3.0 avg return: 6.333333333333333\n",
            "epoch: 3 steps: 30 return: 11.0 avg return: 7.5\n",
            "epoch: 4 steps: 34 return: 4.0 avg return: 5.25\n",
            "epoch: 5 steps: 45 return: 11.0 avg return: 7.25\n",
            "epoch: 6 steps: 49 return: 4.0 avg return: 7.5\n",
            "epoch: 7 steps: 52 return: 3.0 avg return: 5.5\n",
            "epoch: 8 steps: 66 return: 14.0 avg return: 8.0\n",
            "epoch: 9 steps: 75 return: 9.0 avg return: 7.5\n",
            "epoch: 10 steps: 82 return: 7.0 avg return: 8.25\n",
            "epoch: 11 steps: 92 return: 10.0 avg return: 10.0\n",
            "epoch: 12 steps: 95 return: 3.0 avg return: 7.25\n",
            "epoch: 13 steps: 98 return: 3.0 avg return: 5.75\n",
            "epoch: 14 steps: 101 return: 3.0 avg return: 4.75\n",
            "epoch: 15 steps: 110 return: 9.0 avg return: 4.5\n",
            "epoch: 16 steps: 114 return: 4.0 avg return: 4.75\n",
            "epoch: 17 steps: 117 return: 3.0 avg return: 4.75\n",
            "epoch: 18 steps: 124 return: 7.0 avg return: 5.75\n",
            "epoch: 19 steps: 131 return: 7.0 avg return: 5.25\n",
            "epoch: 20 steps: 135 return: 4.0 avg return: 5.25\n",
            "epoch: 21 steps: 140 return: 5.0 avg return: 5.75\n",
            "epoch: 22 steps: 146 return: 6.0 avg return: 5.5\n",
            "epoch: 23 steps: 150 return: 4.0 avg return: 4.75\n",
            "epoch: 24 steps: 155 return: 5.0 avg return: 5.0\n",
            "epoch: 25 steps: 159 return: 4.0 avg return: 4.75\n",
            "epoch: 26 steps: 172 return: 13.0 avg return: 6.5\n",
            "epoch: 27 steps: 185 return: 13.0 avg return: 8.75\n",
            "epoch: 28 steps: 188 return: 3.0 avg return: 8.25\n",
            "epoch: 29 steps: 193 return: 5.0 avg return: 8.5\n",
            "epoch: 30 steps: 197 return: 4.0 avg return: 6.25\n",
            "epoch: 31 steps: 207 return: 10.0 avg return: 5.5\n",
            "epoch: 32 steps: 211 return: 4.0 avg return: 5.75\n",
            "epoch: 33 steps: 216 return: 5.0 avg return: 5.75\n",
            "epoch: 34 steps: 219 return: 3.0 avg return: 5.5\n",
            "epoch: 35 steps: 224 return: 5.0 avg return: 4.25\n",
            "epoch: 36 steps: 230 return: 6.0 avg return: 4.75\n",
            "epoch: 37 steps: 234 return: 4.0 avg return: 4.5\n",
            "epoch: 38 steps: 242 return: 8.0 avg return: 5.75\n",
            "epoch: 39 steps: 246 return: 4.0 avg return: 5.5\n",
            "epoch: 40 steps: 255 return: 9.0 avg return: 6.25\n",
            "epoch: 41 steps: 264 return: 9.0 avg return: 7.5\n",
            "epoch: 42 steps: 267 return: 3.0 avg return: 6.25\n",
            "epoch: 43 steps: 272 return: 5.0 avg return: 6.5\n",
            "epoch: 44 steps: 285 return: 13.0 avg return: 7.5\n",
            "epoch: 45 steps: 289 return: 4.0 avg return: 6.25\n",
            "epoch: 46 steps: 292 return: 3.0 avg return: 6.25\n",
            "epoch: 47 steps: 305 return: 13.0 avg return: 8.25\n",
            "epoch: 48 steps: 313 return: 8.0 avg return: 7.0\n",
            "epoch: 49 steps: 319 return: 6.0 avg return: 7.5\n",
            "epoch: 50 steps: 322 return: 3.0 avg return: 7.5\n",
            "epoch: 51 steps: 326 return: 4.0 avg return: 5.25\n",
            "epoch: 52 steps: 331 return: 5.0 avg return: 4.5\n",
            "epoch: 53 steps: 340 return: 9.0 avg return: 5.25\n",
            "epoch: 54 steps: 352 return: 12.0 avg return: 7.5\n",
            "epoch: 55 steps: 380 return: 28.0 avg return: 13.5\n",
            "epoch: 56 steps: 441 return: 61.0 avg return: 27.5\n",
            "epoch: 57 steps: 455 return: 14.0 avg return: 28.75\n",
            "epoch: 58 steps: 524 return: 69.0 avg return: 43.0\n",
            "epoch: 59 steps: 583 return: 59.0 avg return: 50.75\n",
            "epoch: 60 steps: 646 return: 63.0 avg return: 51.25\n",
            "epoch: 61 steps: 705 return: 59.0 avg return: 62.5\n",
            "epoch: 62 steps: 746 return: 41.0 avg return: 55.5\n",
            "epoch: 63 steps: 845 return: 99.0 avg return: 65.5\n",
            "epoch: 64 steps: 889 return: 44.0 avg return: 60.75\n",
            "epoch: 65 steps: 960 return: 71.0 avg return: 63.75\n",
            "epoch: 66 steps: 1030 return: 70.0 avg return: 71.0\n",
            "epoch: 67 steps: 1094 return: 64.0 avg return: 62.25\n",
            "epoch: 68 steps: 1167 return: 73.0 avg return: 69.5\n",
            "epoch: 69 steps: 1239 return: 72.0 avg return: 69.75\n",
            "epoch: 70 steps: 1304 return: 65.0 avg return: 68.5\n",
            "epoch: 71 steps: 1371 return: 67.0 avg return: 69.25\n",
            "epoch: 72 steps: 1466 return: 95.0 avg return: 74.75\n",
            "epoch: 73 steps: 1534 return: 68.0 avg return: 73.75\n",
            "epoch: 74 steps: 1621 return: 87.0 avg return: 79.25\n",
            "epoch: 75 steps: 1684 return: 63.0 avg return: 78.25\n",
            "epoch: 76 steps: 1765 return: 81.0 avg return: 74.75\n",
            "epoch: 77 steps: 1867 return: 102.0 avg return: 83.25\n",
            "epoch: 78 steps: 1933 return: 66.0 avg return: 78.0\n",
            "epoch: 79 steps: 1999 return: 66.0 avg return: 78.75\n",
            "epoch: 80 steps: 2107 return: 108.0 avg return: 85.5\n",
            "epoch: 81 steps: 2163 return: 56.0 avg return: 74.0\n",
            "epoch: 82 steps: 2219 return: 56.0 avg return: 71.5\n",
            "epoch: 83 steps: 2280 return: 61.0 avg return: 70.25\n",
            "epoch: 84 steps: 2339 return: 59.0 avg return: 58.0\n",
            "epoch: 85 steps: 2395 return: 56.0 avg return: 58.0\n",
            "epoch: 86 steps: 2456 return: 61.0 avg return: 59.25\n",
            "epoch: 87 steps: 2543 return: 87.0 avg return: 65.75\n",
            "epoch: 88 steps: 2599 return: 56.0 avg return: 65.0\n",
            "epoch: 89 steps: 2660 return: 61.0 avg return: 66.25\n",
            "epoch: 90 steps: 2779 return: 119.0 avg return: 80.75\n",
            "epoch: 91 steps: 2842 return: 63.0 avg return: 74.75\n",
            "epoch: 92 steps: 2923 return: 81.0 avg return: 81.0\n",
            "epoch: 93 steps: 2984 return: 61.0 avg return: 81.0\n",
            "epoch: 94 steps: 3038 return: 54.0 avg return: 64.75\n",
            "epoch: 95 steps: 3109 return: 71.0 avg return: 66.75\n",
            "epoch: 96 steps: 3174 return: 65.0 avg return: 62.75\n",
            "epoch: 97 steps: 3221 return: 47.0 avg return: 59.25\n",
            "epoch: 98 steps: 3282 return: 61.0 avg return: 61.0\n",
            "epoch: 99 steps: 3336 return: 54.0 avg return: 56.75\n",
            "epoch: 100 steps: 3390 return: 54.0 avg return: 54.0\n",
            "epoch: 101 steps: 3445 return: 55.0 avg return: 56.0\n",
            "epoch: 102 steps: 3498 return: 53.0 avg return: 54.0\n",
            "epoch: 103 steps: 3575 return: 77.0 avg return: 59.75\n",
            "epoch: 104 steps: 3629 return: 54.0 avg return: 59.75\n",
            "epoch: 105 steps: 3689 return: 60.0 avg return: 61.0\n",
            "epoch: 106 steps: 3749 return: 60.0 avg return: 62.75\n",
            "epoch: 107 steps: 3800 return: 51.0 avg return: 56.25\n",
            "epoch: 108 steps: 3853 return: 53.0 avg return: 56.0\n",
            "epoch: 109 steps: 3909 return: 56.0 avg return: 55.0\n",
            "epoch: 110 steps: 3986 return: 77.0 avg return: 59.25\n",
            "epoch: 111 steps: 4073 return: 87.0 avg return: 68.25\n",
            "epoch: 112 steps: 4123 return: 50.0 avg return: 67.5\n",
            "epoch: 113 steps: 4203 return: 80.0 avg return: 73.5\n",
            "epoch: 114 steps: 4276 return: 73.0 avg return: 72.5\n",
            "epoch: 115 steps: 4334 return: 58.0 avg return: 65.25\n",
            "epoch: 116 steps: 4387 return: 53.0 avg return: 66.0\n",
            "epoch: 117 steps: 4440 return: 53.0 avg return: 59.25\n",
            "epoch: 118 steps: 4492 return: 52.0 avg return: 54.0\n",
            "epoch: 119 steps: 4612 return: 120.0 avg return: 69.5\n",
            "epoch: 120 steps: 4663 return: 51.0 avg return: 69.0\n",
            "epoch: 121 steps: 4762 return: 99.0 avg return: 80.5\n",
            "epoch: 122 steps: 4859 return: 97.0 avg return: 91.75\n",
            "epoch: 123 steps: 4957 return: 98.0 avg return: 86.25\n",
            "epoch: 124 steps: 5015 return: 58.0 avg return: 88.0\n",
            "epoch: 125 steps: 5075 return: 60.0 avg return: 78.25\n",
            "epoch: 126 steps: 5156 return: 81.0 avg return: 74.25\n",
            "epoch: 127 steps: 5226 return: 70.0 avg return: 67.25\n",
            "epoch: 128 steps: 5288 return: 62.0 avg return: 68.25\n",
            "epoch: 129 steps: 5376 return: 88.0 avg return: 75.25\n",
            "epoch: 130 steps: 5449 return: 73.0 avg return: 73.25\n",
            "epoch: 131 steps: 5521 return: 72.0 avg return: 73.75\n",
            "epoch: 132 steps: 5630 return: 109.0 avg return: 85.5\n",
            "epoch: 133 steps: 5763 return: 133.0 avg return: 96.75\n",
            "epoch: 134 steps: 5834 return: 71.0 avg return: 96.25\n",
            "epoch: 135 steps: 5926 return: 92.0 avg return: 101.25\n",
            "epoch: 136 steps: 5994 return: 68.0 avg return: 91.0\n",
            "epoch: 137 steps: 6086 return: 92.0 avg return: 80.75\n",
            "epoch: 138 steps: 6158 return: 72.0 avg return: 81.0\n",
            "epoch: 139 steps: 6239 return: 81.0 avg return: 78.25\n",
            "epoch: 140 steps: 6312 return: 73.0 avg return: 79.5\n",
            "epoch: 141 steps: 6388 return: 76.0 avg return: 75.5\n",
            "epoch: 142 steps: 6511 return: 123.0 avg return: 88.25\n",
            "epoch: 143 steps: 6596 return: 85.0 avg return: 89.25\n",
            "epoch: 144 steps: 6688 return: 92.0 avg return: 94.0\n",
            "epoch: 145 steps: 6769 return: 81.0 avg return: 95.25\n",
            "epoch: 146 steps: 6891 return: 122.0 avg return: 95.0\n",
            "epoch: 147 steps: 6982 return: 91.0 avg return: 96.5\n",
            "epoch: 148 steps: 7195 return: 213.0 avg return: 126.75\n",
            "epoch: 149 steps: 7339 return: 144.0 avg return: 142.5\n",
            "epoch: 150 steps: 7441 return: 102.0 avg return: 137.5\n",
            "epoch: 151 steps: 7567 return: 126.0 avg return: 146.25\n",
            "epoch: 152 steps: 7685 return: 118.0 avg return: 122.5\n",
            "epoch: 153 steps: 7825 return: 140.0 avg return: 121.5\n",
            "epoch: 154 steps: 7926 return: 101.0 avg return: 121.25\n",
            "epoch: 155 steps: 8150 return: 224.0 avg return: 145.75\n",
            "epoch: 156 steps: 8251 return: 101.0 avg return: 141.5\n",
            "epoch: 157 steps: 8436 return: 185.0 avg return: 152.75\n",
            "epoch: 158 steps: 8683 return: 247.0 avg return: 189.25\n",
            "epoch: 159 steps: 8890 return: 207.0 avg return: 185.0\n",
            "epoch: 160 steps: 9890 return: 1000.0 avg return: 409.75\n",
            "epoch: 161 steps: 10890 return: 1000.0 avg return: 613.5\n",
            "epoch: 162 steps: 11890 return: 1000.0 avg return: 801.75\n",
            "epoch: 163 steps: 12890 return: 1000.0 avg return: 1000.0\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-8-d18921bb9858>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m    321\u001b[0m                   critic_network_generator=partial(create_q_network, state_dim=env.observation_space.shape[0],\n\u001b[1;32m    322\u001b[0m                                                    action_dim=env.action_space.shape[0]))\n\u001b[0;32m--> 323\u001b[0;31m     \u001b[0magent\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m200\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-8-d18921bb9858>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self, epochs, environment_steps_before_training, training_steps_per_update, max_environment_steps_per_epoch, pre_sampling_steps, save_models)\u001b[0m\n\u001b[1;32m    299\u001b[0m                 \u001b[0mj\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    300\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0m_\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtraining_steps_per_update\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 301\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlearn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    302\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    303\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-8-d18921bb9858>\u001b[0m in \u001b[0;36mlearn\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    188\u001b[0m         \u001b[0mstates\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mactions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrewards\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstates_prime\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdones\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reply_buffer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msample_batch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    189\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_step_critic\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstates\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mactions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrewards\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstates_prime\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdones\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 190\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_step_actor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstates\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    191\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate_target_weights\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    192\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/tensorflow/python/util/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    148\u001b[0m     \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    149\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 150\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    151\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    152\u001b[0m       \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_process_traceback_frames\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    913\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    914\u001b[0m       \u001b[0;32mwith\u001b[0m \u001b[0mOptionalXlaContext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jit_compile\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 915\u001b[0;31m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    916\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    917\u001b[0m       \u001b[0mnew_tracing_count\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexperimental_get_tracing_count\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    945\u001b[0m       \u001b[0;31m# In this case we have created variables on the first call, so we run the\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    946\u001b[0m       \u001b[0;31m# defunned version which is guaranteed to never create variables.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 947\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_stateless_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# pylint: disable=not-callable\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    948\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_stateful_fn\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    949\u001b[0m       \u001b[0;31m# Release the lock early so that multiple threads can perform the call\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   2451\u001b[0m       (graph_function,\n\u001b[1;32m   2452\u001b[0m        filtered_flat_args) = self._maybe_define_function(args, kwargs)\n\u001b[0;32m-> 2453\u001b[0;31m     return graph_function._call_flat(\n\u001b[0m\u001b[1;32m   2454\u001b[0m         filtered_flat_args, captured_inputs=graph_function.captured_inputs)  # pylint: disable=protected-access\n\u001b[1;32m   2455\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m_call_flat\u001b[0;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[1;32m   1858\u001b[0m         and executing_eagerly):\n\u001b[1;32m   1859\u001b[0m       \u001b[0;31m# No tape is watching; skip to running the function.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1860\u001b[0;31m       return self._build_call_outputs(self._inference_function.call(\n\u001b[0m\u001b[1;32m   1861\u001b[0m           ctx, args, cancellation_manager=cancellation_manager))\n\u001b[1;32m   1862\u001b[0m     forward_backward = self._select_forward_and_backward_functions(\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36mcall\u001b[0;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[1;32m    495\u001b[0m       \u001b[0;32mwith\u001b[0m \u001b[0m_InterpolateFunctionError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    496\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mcancellation_manager\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 497\u001b[0;31m           outputs = execute.execute(\n\u001b[0m\u001b[1;32m    498\u001b[0m               \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msignature\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    499\u001b[0m               \u001b[0mnum_outputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_num_outputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/tensorflow/python/eager/execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     52\u001b[0m   \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     53\u001b[0m     \u001b[0mctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mensure_initialized\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 54\u001b[0;31m     tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,\n\u001b[0m\u001b[1;32m     55\u001b[0m                                         inputs, attrs, num_outputs)\n\u001b[1;32m     56\u001b[0m   \u001b[0;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    }
  ]
}